{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05613b7-10fb-40f2-935e-49aac9b8ecfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 100 frames from ADIPTO.mp4.\n",
      "Extracted 100 frames from ALEX.mp4.\n",
      "Extracted 100 frames from Ankit.mp4.\n",
      "Extracted 100 frames from biswasda.mp4.\n",
      "Extracted 100 frames from DEEP.mp4.\n",
      "Extracted 100 frames from DEEPMALA.mp4.\n",
      "Extracted 100 frames from DW.mp4.\n",
      "Extracted 100 frames from HARDIK.mp4.\n",
      "Extracted 100 frames from HRITTIKA.mp4.\n",
      "Extracted 100 frames from kharo.mp4.\n",
      "Extracted 100 frames from KRISH.mp4.\n",
      "Extracted 100 frames from MAYUKH.mp4.\n",
      "Extracted 100 frames from PRITHA.mp4.\n",
      "Extracted 100 frames from raktim.mp4.\n",
      "Extracted 100 frames from RAVI_MARJIT.mp4.\n",
      "Extracted 100 frames from Rounak.mp4.\n",
      "Extracted 100 frames from RUP.mp4.\n",
      "Extracted 100 frames from SANDY.mp4.\n",
      "Extracted 100 frames from soumesh.mp4.\n",
      "Extracted 100 frames from sourav.mp4.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "\n",
    "def extract_random_frames(master_folder, output_folder, frame_count=100):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterate through all videos in the master folder\n",
    "    for video_file in os.listdir(master_folder):\n",
    "        video_path = os.path.join(master_folder, video_file)\n",
    "\n",
    "        # Ensure it's a video file\n",
    "        if not video_file.lower().endswith(('.mp4', '.avi', '.mov', '.mkv')):\n",
    "            continue\n",
    "\n",
    "        # Create a folder with the same name as the video (without extension)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        video_output_folder = os.path.join(output_folder, video_name)\n",
    "        os.makedirs(video_output_folder, exist_ok=True)\n",
    "\n",
    "        # Open the video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Ensure the video has enough frames\n",
    "        if total_frames < frame_count:\n",
    "            print(f\"Skipping {video_file}, not enough frames.\")\n",
    "            cap.release()\n",
    "            continue\n",
    "\n",
    "        # Select 100 unique random frames\n",
    "        random_frames = random.sample(range(total_frames), frame_count)\n",
    "        random_frames.sort()  # Ensure the frames are in order\n",
    "\n",
    "        frame_idx = 0\n",
    "        extracted = 0\n",
    "\n",
    "        while extracted < frame_count and cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            if frame_idx in random_frames:\n",
    "                frame_path = os.path.join(video_output_folder, f\"frame_{frame_idx:05d}.jpg\")\n",
    "                cv2.imwrite(frame_path, frame)\n",
    "                extracted += 1\n",
    "\n",
    "            frame_idx += 1\n",
    "\n",
    "        cap.release()\n",
    "        print(f\"Extracted {extracted} frames from {video_file}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    master_folder = r\"C:\\Users\\sande\\Desktop\\final_year_project\\video_data\"  # Replace with the path to your master folder\n",
    "    output_folder = r\"C:\\Users\\sande\\Desktop\\final_year_project\\phase3\"  # Replace with the path to your output folder\n",
    "\n",
    "    extract_random_frames(master_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1b033-1b62-48ae-b38f-7c4ca72e9981",
   "metadata": {},
   "source": [
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc589f68-1a56-4c04-94d0-1f79c2fd7519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and encoding faces...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 20/20 [23:20<00:00, 70.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to C:\\Users\\sande\\Desktop\\final_year_project\\face_recognition_model_phase3.pkl\n",
      "Error processing image at C:\\Users\\sande\\Desktop\\final_year_project\\Screenshot 2025-02-10 164403.png: [Errno 2] No such file or directory: 'C:\\\\Users\\\\sande\\\\Desktop\\\\final_year_project\\\\Screenshot 2025-02-10 164403.png'\n",
      "Predicted person: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "class FaceRecognitionSystem:\n",
    "    def __init__(self, data_dir=None, model_path=None):\n",
    "        \"\"\"\n",
    "        Initialize the face recognition system.\n",
    "\n",
    "        :param data_dir: Path to the master directory containing subfolders for each person.\n",
    "        :param model_path: Path to save or load the trained model.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.model_path = model_path\n",
    "        self.known_face_encodings = []\n",
    "        self.known_face_names = []\n",
    "\n",
    "    def load_and_encode_faces(self):\n",
    "        \"\"\"\n",
    "        Load images from the directory structure and encode faces.\n",
    "        \"\"\"\n",
    "        print(\"Loading and encoding faces...\")\n",
    "\n",
    "        for person in tqdm(os.listdir(self.data_dir)):\n",
    "            person_dir = os.path.join(self.data_dir, person)\n",
    "            if not os.path.isdir(person_dir):\n",
    "                continue\n",
    "\n",
    "            for img_name in os.listdir(person_dir):\n",
    "                if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(person_dir, img_name)\n",
    "\n",
    "                try:\n",
    "                    image = face_recognition.load_image_file(img_path)\n",
    "                    face_encodings = face_recognition.face_encodings(image)\n",
    "\n",
    "                    if face_encodings:\n",
    "                        self.known_face_encodings.append(face_encodings[0])\n",
    "                        self.known_face_names.append(person)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file.\n",
    "        \"\"\"\n",
    "        if not self.model_path:\n",
    "            raise ValueError(\"Model path is not specified.\")\n",
    "\n",
    "        model_data = {\n",
    "            'encodings': self.known_face_encodings,\n",
    "            'names': self.known_face_names\n",
    "        }\n",
    "\n",
    "        with open(self.model_path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved to {self.model_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"\n",
    "        Load the trained model from a file.\n",
    "        \"\"\"\n",
    "        if not self.model_path:\n",
    "            raise ValueError(\"Model path is not specified.\")\n",
    "\n",
    "        with open(self.model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.known_face_encodings = model_data['encodings']\n",
    "        self.known_face_names = model_data['names']\n",
    "\n",
    "        print(f\"Model loaded from {self.model_path}\")\n",
    "\n",
    "    def predict_face(self, image_path, tolerance=0.6):\n",
    "        \"\"\"\n",
    "        Predict the identity of a person in a new image.\n",
    "\n",
    "        :param image_path: Path to the new image.\n",
    "        :param tolerance: Distance threshold for face matching. Lower is stricter.\n",
    "        :return: Predicted person name or \"Unknown\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            unknown_image = face_recognition.load_image_file(image_path)\n",
    "            face_encodings = face_recognition.face_encodings(unknown_image)\n",
    "\n",
    "            if not face_encodings:\n",
    "                return \"No face detected\"\n",
    "\n",
    "            for face_encoding in face_encodings:\n",
    "                distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(distances)\n",
    "\n",
    "                if distances[best_match_index] <= tolerance:\n",
    "                    return self.known_face_names[best_match_index]\n",
    "\n",
    "            return \"Unknown\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image at {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def run_complete_pipeline(self, new_image_path=None):\n",
    "        \"\"\"\n",
    "        Run the complete pipeline from data loading to prediction.\n",
    "\n",
    "        :param new_image_path: Path to the new image for prediction.\n",
    "        \"\"\"\n",
    "        if self.data_dir:\n",
    "            self.load_and_encode_faces()\n",
    "            if self.model_path:\n",
    "                self.save_model()\n",
    "\n",
    "        if new_image_path:\n",
    "            prediction = self.predict_face(new_image_path)\n",
    "            print(f\"Predicted person: {prediction}\")\n",
    "            return prediction\n",
    "\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = r\"C:\\Users\\sande\\Desktop\\final_year_project\\phase3\"  \n",
    "    model_path = r\"C:\\Users\\sande\\Desktop\\final_year_project\\face_recognition_model_phase3.pkl\"\n",
    "    test_image_path = r\"C:\\Users\\sande\\Desktop\\final_year_project\\Screenshot 2025-02-10 164403.png\"\n",
    "\n",
    "    face_recognition_system = FaceRecognitionSystem(data_dir=data_dir, model_path=model_path)\n",
    "    face_recognition_system.run_complete_pipeline(new_image_path=test_image_path)\n",
    "\n",
    "    # Later, you can load the model and predict using the following code:\n",
    "    # face_recognition_system = FaceRecognitionSystem(model_path=model_path)\n",
    "    # face_recognition_system.load_model()\n",
    "    # prediction = face_recognition_system.predict_face(test_image_path)\n",
    "    # print(f\"Predicted person: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9bc83e-79e7-420c-b6a5-aa3afc880950",
   "metadata": {},
   "source": [
    "#cropping of faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d446c9-391c-4224-a2b7-d494291fc5f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo faces were detected in the image.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 68\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# Ask for the image path\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter the path to the group photo: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m# Validate image path\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(image_path):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\ipykernel\\kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def crop_faces(image_path, output_folder=\"cropped_faces\", min_face_size=(30, 30)):\n",
    "    \"\"\"\n",
    "    Detect and crop faces from an image, saving each face as a separate file.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the input image\n",
    "        output_folder (str): Folder to save cropped faces\n",
    "        min_face_size (tuple): Minimum size of face to detect (width, height)\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of faces detected and saved\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not read image at {image_path}\")\n",
    "        return 0\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Load the pre-trained face detector\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=min_face_size\n",
    "    )\n",
    "    \n",
    "    print(f\"Found {len(faces)} faces!\")\n",
    "    \n",
    "    # Generate timestamp for unique filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Crop and save each face\n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        # Add some margin around the face\n",
    "        margin = int(min(w, h) * 0.2)\n",
    "        x_margin = max(0, x - margin)\n",
    "        y_margin = max(0, y - margin)\n",
    "        w_margin = min(image.shape[1] - x_margin, w + 2 * margin)\n",
    "        h_margin = min(image.shape[0] - y_margin, h + 2 * margin)\n",
    "        \n",
    "        # Crop the face\n",
    "        face = image[y_margin:y_margin + h_margin, x_margin:x_margin + w_margin]\n",
    "        \n",
    "        # Save the cropped face\n",
    "        face_filename = os.path.join(output_folder, f\"face_{timestamp}_{i+1}.jpg\")\n",
    "        cv2.imwrite(face_filename, face)\n",
    "        print(f\"Saved face {i+1} to {face_filename}\")\n",
    "    \n",
    "    return len(faces)\n",
    "\n",
    "def main():\n",
    "    # Ask for the image path\n",
    "    image_path = input(\"Enter the path to the group photo: \")\n",
    "    \n",
    "    # Validate image path\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: The file '{image_path}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    # Process the image\n",
    "    num_faces = crop_faces(image_path)\n",
    "    \n",
    "    if num_faces > 0:\n",
    "        print(f\"Successfully saved {num_faces} faces to the 'cropped_faces' folder.\")\n",
    "    else:\n",
    "        print(\"No faces were detected in the image.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5eb836-546a-449d-8db6-7eb5280309ef",
   "metadata": {},
   "source": [
    "PREDICTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "891b6ddf-8f61-4253-a6c0-bbee36739507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: 18866369661.jpg\n",
      "Predicted person in 18866369661.jpg: SANDY\n",
      "Processing image: 20240323152327_IMG_5700.JPG\n",
      "No face detected in 20240323152327_IMG_5700.JPG\n",
      "Processing image: 20240324080456_IMG_5791.JPG\n",
      "No face detected in 20240324080456_IMG_5791.JPG\n",
      "Processing image: 20240325122852_IMG_5822.JPG\n",
      "Predicted person in 20240325122852_IMG_5822.JPG: ADIPTO\n",
      "Processing image: face_20250301_135904_2.jpg\n",
      "Predicted person in face_20250301_135904_2.jpg: KRISH\n",
      "Processing image: face_20250301_135904_3.jpg\n",
      "Predicted person in face_20250301_135904_3.jpg: RUP\n",
      "Processing image: face_20250301_135904_4.jpg\n",
      "Predicted person in face_20250301_135904_4.jpg: SANDY\n",
      "Processing image: face_20250301_135904_5.jpg\n",
      "Predicted person in face_20250301_135904_5.jpg: raktim\n",
      "Processing image: face_20250301_135904_6.jpg\n",
      "Predicted person in face_20250301_135904_6.jpg: biswasda\n",
      "Processing image: face_20250301_135904_7.jpg\n",
      "Predicted person in face_20250301_135904_7.jpg: kharo\n",
      "Processing image: face_20250301_140020_1.jpg\n",
      "No face detected in face_20250301_140020_1.jpg\n",
      "Processing image: face_20250301_140020_10.jpg\n",
      "No face detected in face_20250301_140020_10.jpg\n",
      "Processing image: face_20250301_140020_11.jpg\n",
      "Predicted person in face_20250301_140020_11.jpg: SANDY\n",
      "Processing image: face_20250301_140020_12.jpg\n",
      "Predicted person in face_20250301_140020_12.jpg: ADIPTO\n",
      "Processing image: face_20250301_140020_13.jpg\n",
      "No face detected in face_20250301_140020_13.jpg\n",
      "Processing image: face_20250301_140020_14.jpg\n",
      "Predicted person in face_20250301_140020_14.jpg: raktim\n",
      "Processing image: face_20250301_140020_15.jpg\n",
      "No face detected in face_20250301_140020_15.jpg\n",
      "Processing image: face_20250301_140020_16.jpg\n",
      "No face detected in face_20250301_140020_16.jpg\n",
      "Processing image: face_20250301_140020_17.jpg\n",
      "Predicted person in face_20250301_140020_17.jpg: raktim\n",
      "Processing image: face_20250301_140020_18.jpg\n",
      "Predicted person in face_20250301_140020_18.jpg: SANDY\n",
      "Processing image: face_20250301_140020_19.jpg\n",
      "No face detected in face_20250301_140020_19.jpg\n",
      "Processing image: face_20250301_140020_2.jpg\n",
      "Predicted person in face_20250301_140020_2.jpg: SANDY\n",
      "Processing image: face_20250301_140020_20.jpg\n",
      "No face detected in face_20250301_140020_20.jpg\n",
      "Processing image: face_20250301_140020_21.jpg\n",
      "Predicted person in face_20250301_140020_21.jpg: SANDY\n",
      "Processing image: face_20250301_140020_22.jpg\n",
      "No face detected in face_20250301_140020_22.jpg\n",
      "Processing image: face_20250301_140020_3.jpg\n",
      "No face detected in face_20250301_140020_3.jpg\n",
      "Processing image: face_20250301_140020_4.jpg\n",
      "Predicted person in face_20250301_140020_4.jpg: HRITTIKA\n",
      "Processing image: face_20250301_140020_5.jpg\n",
      "Predicted person in face_20250301_140020_5.jpg: ADIPTO\n",
      "Processing image: face_20250301_140020_6.jpg\n",
      "No face detected in face_20250301_140020_6.jpg\n",
      "Processing image: face_20250301_140020_7.jpg\n",
      "No face detected in face_20250301_140020_7.jpg\n",
      "Processing image: face_20250301_140020_8.jpg\n",
      "Predicted person in face_20250301_140020_8.jpg: PRITHA\n",
      "Processing image: face_20250301_140020_9.jpg\n",
      "Predicted person in face_20250301_140020_9.jpg: HARDIK\n",
      "Processing image: face_20250301_143810_1.jpg\n",
      "Predicted person in face_20250301_143810_1.jpg: RUP\n",
      "Processing image: face_20250301_143810_10.jpg\n",
      "Predicted person in face_20250301_143810_10.jpg: SANDY\n",
      "Processing image: face_20250301_143810_11.jpg\n",
      "Predicted person in face_20250301_143810_11.jpg: DEEPMALA\n",
      "Processing image: face_20250301_143810_12.jpg\n",
      "Predicted person in face_20250301_143810_12.jpg: ALEX\n",
      "Processing image: face_20250301_143810_2.jpg\n",
      "Predicted person in face_20250301_143810_2.jpg: Rounak\n",
      "Processing image: face_20250301_143810_3.jpg\n",
      "Predicted person in face_20250301_143810_3.jpg: HARDIK\n",
      "Processing image: face_20250301_143810_4.jpg\n",
      "Predicted person in face_20250301_143810_4.jpg: HRITTIKA\n",
      "Processing image: face_20250301_143810_5.jpg\n",
      "Predicted person in face_20250301_143810_5.jpg: Ankit\n",
      "Processing image: face_20250301_143810_6.jpg\n",
      "Predicted person in face_20250301_143810_6.jpg: KRISH\n",
      "Processing image: face_20250301_143810_7.jpg\n",
      "Predicted person in face_20250301_143810_7.jpg: ADIPTO\n",
      "Processing image: face_20250301_143810_8.jpg\n",
      "Predicted person in face_20250301_143810_8.jpg: raktim\n",
      "Processing image: face_20250301_143810_9.jpg\n",
      "Predicted person in face_20250301_143810_9.jpg: ADIPTO\n",
      "Processing image: Screenshot 2025-02-10 164020.png\n",
      "No face detected in Screenshot 2025-02-10 164020.png\n",
      "Processing image: Screenshot 2025-02-10 164137.png\n",
      "Predicted person in Screenshot 2025-02-10 164137.png: RUP\n",
      "Processing image: Screenshot 2025-02-10 164310.png\n",
      "Predicted person in Screenshot 2025-02-10 164310.png: DEEPMALA\n",
      "Processing image: Screenshot 2025-02-10 164403.png\n",
      "Predicted person in Screenshot 2025-02-10 164403.png: DEEPMALA\n",
      "Processing image: Screenshot 2025-02-28 204928.png\n",
      "Predicted person in Screenshot 2025-02-28 204928.png: HARDIK\n",
      "Processing image: Screenshot 2025-02-28 210740.png\n",
      "Predicted person in Screenshot 2025-02-28 210740.png: DEEP\n",
      "Processing image: Screenshot 2025-02-28 211010.png\n",
      "Predicted person in Screenshot 2025-02-28 211010.png: KRISH\n",
      "Processing image: Screenshot 2025-02-28 211139.png\n",
      "Predicted person in Screenshot 2025-02-28 211139.png: ALEX\n",
      "Processing image: Screenshot 2025-02-28 211502.png\n",
      "Predicted person in Screenshot 2025-02-28 211502.png: Rounak\n",
      "Processing image: Screenshot 2025-02-28 215034.png\n",
      "Predicted person in Screenshot 2025-02-28 215034.png: HRITTIKA\n",
      "Processing image: Screenshot 2025-02-28 215200.png\n",
      "Predicted person in Screenshot 2025-02-28 215200.png: MAYUKH\n",
      "Processing image: Screenshot 2025-02-28 230005.png\n",
      "Predicted person in Screenshot 2025-02-28 230005.png: raktim\n",
      "Processing image: Screenshot 2025-02-28 230026.png\n",
      "Predicted person in Screenshot 2025-02-28 230026.png: raktim\n",
      "Processing image: Screenshot 2025-02-28 230117.png\n",
      "Predicted person in Screenshot 2025-02-28 230117.png: kharo\n",
      "Processing image: Screenshot 2025-02-28 230141.png\n",
      "Predicted person in Screenshot 2025-02-28 230141.png: Ankit\n",
      "Processing image: Screenshot 2025-02-28 230423.png\n",
      "Predicted person in Screenshot 2025-02-28 230423.png: biswasda\n",
      "Processing image: Screenshot 2025-02-28 230515.png\n",
      "Predicted person in Screenshot 2025-02-28 230515.png: PRITHA\n",
      "Processing image: Screenshot 2025-03-01 142047.png\n",
      "Predicted person in Screenshot 2025-03-01 142047.png: DW\n",
      "Processing image: Screenshot 2025-03-01 142055.png\n",
      "Predicted person in Screenshot 2025-03-01 142055.png: DW\n",
      "Processing image: Screenshot 2025-03-01 142100.png\n",
      "Predicted person in Screenshot 2025-03-01 142100.png: DW\n",
      "Processing image: Screenshot 2025-03-01 142107.png\n",
      "Predicted person in Screenshot 2025-03-01 142107.png: DW\n",
      "Processing image: Screenshot 2025-03-01 142233.png\n",
      "No face detected in Screenshot 2025-03-01 142233.png\n",
      "Processing image: Screenshot 2025-03-01 142244.png\n",
      "Predicted person in Screenshot 2025-03-01 142244.png: soumesh\n",
      "Processing image: Screenshot 2025-03-01 142247.png\n",
      "Predicted person in Screenshot 2025-03-01 142247.png: soumesh\n",
      "Processing image: Screenshot 2025-03-01 142253.png\n",
      "Predicted person in Screenshot 2025-03-01 142253.png: soumesh\n",
      "Processing image: Screenshot 2025-03-01 142301.png\n",
      "No face detected in Screenshot 2025-03-01 142301.png\n",
      "Processing image: Screenshot 2025-03-01 142322.png\n",
      "Predicted person in Screenshot 2025-03-01 142322.png: RAVI_MARJIT\n",
      "Processing image: Screenshot 2025-03-01 142329.png\n",
      "Predicted person in Screenshot 2025-03-01 142329.png: RAVI_MARJIT\n",
      "Processing image: Screenshot 2025-03-01 142335.png\n",
      "Predicted person in Screenshot 2025-03-01 142335.png: RAVI_MARJIT\n",
      "Processing image: Screenshot 2025-03-01 142339.png\n",
      "Predicted person in Screenshot 2025-03-01 142339.png: RAVI_MARJIT\n",
      "Processing image: Screenshot 2025-03-01 142345.png\n",
      "No face detected in Screenshot 2025-03-01 142345.png\n",
      "Processing image: Screenshot 2025-03-01 142520.png\n",
      "Predicted person in Screenshot 2025-03-01 142520.png: sourav\n",
      "Processing image: Screenshot 2025-03-01 142529.png\n",
      "Predicted person in Screenshot 2025-03-01 142529.png: sourav\n",
      "Processing image: Screenshot 2025-03-01 142534.png\n",
      "Predicted person in Screenshot 2025-03-01 142534.png: sourav\n",
      "Processing image: Screenshot 2025-03-01 142538.png\n",
      "Predicted person in Screenshot 2025-03-01 142538.png: sourav\n",
      "Processing image: WhatsApp Image 2025-02-28 at 21.24.28_cd73e5d5.jpg\n",
      "Predicted person in WhatsApp Image 2025-02-28 at 21.24.28_cd73e5d5.jpg: ADIPTO\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "def predict_and_display_images(model_path, test_image_folder, tolerance=0.6):\n",
    "    \"\"\"\n",
    "    Predict the identity of a person in images from a folder and display the results.\n",
    "\n",
    "    :param model_path: Path to the trained model file.\n",
    "    :param test_image_folder: Path to the folder containing test images.\n",
    "    :param tolerance: Distance threshold for face matching. Lower is stricter.\n",
    "    \"\"\"\n",
    "    # Load the trained model\n",
    "    with open(model_path, 'rb') as f:\n",
    "        model_data = pickle.load(f)\n",
    "    \n",
    "    known_face_encodings = model_data['encodings']\n",
    "    known_face_names = model_data['names']\n",
    "\n",
    "    # Iterate through all images in the test folder\n",
    "    for img_name in os.listdir(test_image_folder):\n",
    "        if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            continue  # Skip non-image files\n",
    "\n",
    "        img_path = os.path.join(test_image_folder, img_name)\n",
    "        print(f\"Processing image: {img_name}\")\n",
    "\n",
    "        try:\n",
    "            # Load the image\n",
    "            image = face_recognition.load_image_file(img_path)\n",
    "            face_locations = face_recognition.face_locations(image)\n",
    "            face_encodings = face_recognition.face_encodings(image, face_locations)\n",
    "\n",
    "            if not face_encodings:\n",
    "                print(f\"No face detected in {img_name}\")\n",
    "                continue\n",
    "\n",
    "            # Convert the image to BGR format for OpenCV display\n",
    "            image_display = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            # Loop through each face found in the image\n",
    "            for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "                # Compare the face with known faces\n",
    "                distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                best_match_index = np.argmin(distances)\n",
    "\n",
    "                if distances[best_match_index] <= tolerance:\n",
    "                    name = known_face_names[best_match_index]\n",
    "                else:\n",
    "                    name = \"Unknown\"\n",
    "                    \n",
    "\n",
    "                # Draw a rectangle around the face\n",
    "                cv2.rectangle(image_display, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "                # Draw the predicted name below the face\n",
    "                #cv2.putText(image_display, name, (left, bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "                cv2.putText(image_display, name, (left, bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 255), 2)\n",
    "\n",
    "                print(f\"Predicted person in {img_name}: {name}\")\n",
    "\n",
    "            # Display the image with predictions\n",
    "            cv2.imshow(\"Predicted Image\", image_display)\n",
    "            cv2.waitKey(0)  # Wait for a key press to move to the next image\n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {e}\")\n",
    "\n",
    "# Example usage\n",
    "model_path = r\"C:\\Users\\sande\\Desktop\\final_year_project\\face_recognition_model_phase3.pkl\"\n",
    "test_image_folder = r\"C:\\Users\\sande\\Desktop\\final_year_project\\test_image folder\"\n",
    "predict_and_display_images(model_path, test_image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0708405f-1ac8-4d06-ab5e-db96e8d7093b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
